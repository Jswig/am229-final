\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage[ruled]{algorithm2e}
\usepackage{mathpb}

\author{Anders Poirel, \\ AM 229, Fall 2020}
\date{\today}
\title{
    Distributed Support Vector Machines via the Alternating Direction Method of Multipliers
}

\begin{document}

\maketitle 

\section{Alternating Descent Method of Multipliers}

\subsection{Motivation}

The \emph{Alternating Descent Method of Multipliers} (ADMM) algorithm is motivated by a 
desire to combine the best properties of two simpler algorithms while overcoming 
their respective limitations \cite{boydistributed}. \\
On one hand, the \emph{dual descent} algorithm has subproblems that can be easily distributed. 
However, this comes at the cost of strong assumptions on the form of the problem. 
On the other hand, the \emph{method of multipliers} (MM) modifies dual ascent to relax the required assumptions, but this sacrifies the ability to distribute parts of the algorithm. 

\subsection{Algorithm}

The ADMM algorithm solves problems of the form
\begin{equation} \label{admmform}
\begin{aligned}
    \min_{x, z} \quad & f(x) + g(z) \\
    \textrm{subj. to } \quad & Ax + Bz = c
\end{aligned}
\end{equation}
where $x \in \R^m, z \in \R^n, A \in R^{m \times p}, B \in \R^{n \times p}, c \in \R^p$

Like MM, the ADMM algorithm in practice solves an equivalent augmented problem 
\begin{equation} \label{admm_aug_form}
    \begin{aligned}
        \min_{x, z} \quad & f(x) + g(z) + \frac{\rho}{2} \|Ax + Bz - c\|_2^2\\
        \textrm{subj. to } \quad & Ax + Bz = c
    \end{aligned}
    \end{equation}
Indeed, any satisfiable pair $x,z$ zeroes out the extra term in the objective, 
recovering problem (\ref{admmform}). \\
The associated augmented Lagrangian is
\[
    L_{\rho}(x,z,\nu) =  f(x) + g(z) + \nu^\top(Ax+Bz+c) + \frac{\rho}{2} \|Ax + Bz - c\|_2^2
\]

The algorithm is given by  \\

\begin{algorithm}[H]
    \caption{ADMM}

    \SetAlgoNoLine

    \For{$k=1,2, \ldots$}{
        $ \displaystyle 
            x_{k+1} \leftarrow \argmin_x  L_{\rho}(x,z_k,\nu_k)
        $
        
        $ \displaystyle 
        x_{k+1} \leftarrow \argmin_z  L_{\rho}(x_{k+1},z,\nu_k)
        $

        $ \displaystyle 
        \nu_{k+1} \leftarrow \nu_k + \rho(Ax_{k+1} + Bz_{k+1} - c)
        $
    }
\end{algorithm}

\subsection{Properties of ADMM}

\quad \textbf{Convergence}\\
A major advantage of ADMM is that it converges in many common scenario
where dual ascent does not. For instance, dual ascent does not converge if the objective is 
linear \cite{boydistributed}. In fact if the problem satisfies the following 
assumptions
\begin{itemize}
    \item[(1)] $\rm{epi}(f)$ and $\rm{epi}(g)$ are closed, nonempty convex sets 
    \item[(2)] The Lagrangian of problem formulation (\ref{admmform}) has a saddle point  
\end{itemize}
Then 
that is, the points approach feasability and the objective approaches its optimum

\begin{equation*}
\begin{aligned}
    Ax_k + Bz_k &\rightarrow 0 \text{ as } k \rightarrow \infty \\
    f(x_k) + g(z_k) &\rightarrow p^* \text{ as } k \rightarrow \infty
\end{aligned}
\end{equation*}

These assumptions hold in many applications, including the example
discussed in a later section  \cite{boydistributed}. \\

\textbf{Seperability}

The function $f$ is \emph{seperable} if
\[
    f(x) = \sum_{i=1}^n f+i(x_i)  
\]
where $x = (x_1, \ldots, x_n)$ and $x_i$ are subvectors of $x$. If either $f$ or $g$ 
in the ADMM problem (\ref{admmform}) are seperable, then the $x$ and $z$- minimization 
steps in the algorithm can  be decomposed into independant problems \cite{boydistributed}, which allows 
the algorithm to conduct these updates in parallel, yielding distributed solvers 
for many problems.
% For instance, if $f$ is seperable, letting $A = [A_1 \cdots A_n]$ the $x$-minization step becomes 
% \[
%     x_{k+1} \leftarrow \argmin_x  \left( 
%         \sum_{i=1}^N  \left[ f_i(x_i) +\nu^\top A_ix_i \right] + g(z) + \nu^\top(Bz+c) + \frac{\rho}{2} \|Ax + Bz - c\|_2^2 
%         \right)
% \]
% which is equivalent to, for $i = 1, \ldots, N$
% \[
%     (x_i)_{k+1} \leftarrow \argmin_{x_i} f_i(x_i) + \nu^\top A_ix_i  + \frac{1}{N} 
% \]
% (this can be show by minimizing over components of $x$ except $x_i$)
% We can carry out the $x_i$
 
\section{Distributed ADMM}

\subsection*{Global variable consensus ADMM}

The power of the ADMM becomes clear when the objective of the minimization problem is 
\textit{additive}, i.e. when it is of the form 

\begin{equation} \label{additive}
    \min \quad \sum_{i=1}^N f_i(x)
\end{equation}
    
where each of the $f_i$ are convex. In this situation, one can derive a version of the ADMM 
algorithm that can be distributed, which can yield a sizeable improvement in performance. 
To obtain a distributed algorithm, the objective function should be \emph{seperable} \\

Now, to transform an addditive objective into a seperable objective, the problem 
can be rewritten as a \emph{global consensus problem} with 
``local variables'' which are all constrained to be equal to the original
variable $x$ \cite{nedic201010}, 

\begin{equation}
\begin{aligned}
    \min \quad & \sum_{i=1}^N f_i(x_i) \\
    \textrm{subj. to } \quad & x_i = z \quad \forall i = 1, \ldots, n
\end{aligned}
\end{equation}

In many situations, the objective has an extra term in $x$, which is rewritten in 
terms of $z$ 

\begin{equation} \label{gen_seperable}
\begin{aligned}
        \min \quad & \sum_{i=1}^N f_i(x_i) + g(z)\\
        \textrm{subj. to } \quad & x_i = z \quad \forall i = 1, \ldots, n
\end{aligned}
\end{equation}

The augmented Lagrangian is then 
\[ 
    L_\rho(x_1, \ldots, x_N, z, \nu) = 
    g(z) + \sum_{i=1}^N \left( f_i(x_i) + (\nu_i)_k^\top(x_i - z) + \frac{\rho}{2} \|x_i - z\|_2^2 \right)
\]
which yields the following ADMM algorithm after simplifying the z-update step \cite{boydistributed}:

\begin{algorithm}[H]
    \caption{Global variable consensus ADMM}

    \SetAlgoNoLine

    \For{$k=1,2, \ldots$}{
        $ \displaystyle
            (x_i)_{k+1}  \leftarrow \argmin_{x_i} \left(
                f_i(x_i) + (\nu_i)_k^\top (x_i - z_k) + \frac{\rho}{2} \|x_i -z_k \|_2^2
            \right) 
        $

        $ \displaystyle
            z_{k+1}  \leftarrow  \argmin_z \left( 
                g(z) + \frac{N\rho}{2} \|z - \overline{x}_{k+1} - (1/ \rho) \overline{\nu}_k\|_2^2
            \right) 
        $

        $ \displaystyle
            (\nu_i)_{k+1} \leftarrow (\nu_i)_k  + \rho(  (\beta_i)_k - z_{k+1})
        $
    }
\end{algorithm}
\vspace{5pt}
This algorithm performs $N$ distinct $x_i$ and $\nu_i$-updates at each step, each of which is 
can be done in parallel as they are independant of one-another. The $z$-update is handled by a central 
process, and coordinates the solutions of the subproblems solved in the $x, \nu$ updates.

\subsection{Regularized Model Estimation}

The ADMM form (\ref{admmform}) is particularly natural for regularized model estimation problems, 
where the objective can be written as the sum of a ``loss'' and a ``penalty'' term, i.e.
\[
    \min_{\beta} \quad {l(\beta, X, y) + r(\beta)}
\]
where $X \in \R^{m \times p}$ is a matrix of training examples, $y \in \R^m$ is the vector of training
labels, and $\beta$ is a vector of model parameters. If the loss function $l$ and regularization function 
$r$ are respectively convex, the substitution $\beta = z$ immediately yields a problem in desired form (\ref{admmform})

\begin{equation}
\begin{aligned}
    \min_{\beta, z} \quad & {l(\beta, X, y) + r(z)} \\
    \textrm{subj. to} \quad & \beta - z = 0
\end{aligned}
\end{equation}

When $l$ is seperable, this problem can be solved using the global consensus ADMM algorithm by splitting
the problem over blocks of data \cite{boydistributed}, such that
\[
    X = \begin{bmatrix} X_1 \\ \vdots \\ X_N \end{bmatrix}, \quad
    y = \begin{bmatrix} y_1, \\ \vdots \\ y_n\end{bmatrix}
\]
where $X_i \in \R^{m_i \times p}$, $y \in \R^{m_i}$, $\sum_{i=1}^n m_i = m$, and $l_i$ is 
the loss functions on the $i^{th}$ block of data.
The algorithm is then, following the result for problems of the form (\ref{gen_seperable})
and using uses the more compact scaled form described in \cite{boydistributed},\\

\begin{algorithm}[H]
    \caption{Consensus global variable ADMM for regularizated model estimation}

    \SetAlgoNoLine

    \For{$k=1,2, \ldots$}{
        $ \displaystyle
            (\beta_i)_{k+1}  \leftarrow \argmin_{\beta_i} \left(
                (l_i(\beta_i, X_i, y_i)+ (\rho/2) \|\beta_i - z_k + (u_i)_k \|_2^2 
            \right) 
        $
        $ \displaystyle
            z_{k+1}  \leftarrow  \argmin_z \left( 
                r(z) + \frac{N \rho}{2} \|z\|_2^2  - \overline{\beta}_{k+1} - \overline{u}_k
            \right) 
        $
        $ \displaystyle
            (u_i)_{k+1}  \leftarrow (u_i)_k + (\beta_i)_k - z_{k+1}
        $
        }
\end{algorithm}
\vspace{5pt}
The next section demonstrates a worked out example of a straighforward application of this algorithm.

\section{Support Vector Classifiers}


\subsection{Problem formulation}

Letting $x_1, \ldots x_m \in \R^p$ be the training examples, and $y_1, \ldots, y_N \in \{-1,1\}$ be the training labels, 
the standard form of the binary \emph{support vector classifer} (SVC) problem is given in   \cite{hastie2009elements} by

\begin{equation} \label{first_form}
\begin{aligned}
    \argmin_{\beta, \beta_0, \xi}  \quad 
    & \|\beta\|^2_2 & \\
    \textrm{subj. to} \quad 
    & y_i (x_i^\top \beta + \beta_0) \geq 1 - \xi & \forall i = 1, \ldots, m \\
    & \xi_i \geq 0  & \forall i = 1, \ldots, m \\
    & \sum_{i=1}^m \xi_i \leq C & C \textrm{ constant}
\end{aligned}
\end{equation}

Equivalently, we can formulate this as 

\begin{equation} \label{second_form}
\begin{aligned}
    \argmin_{\beta, \beta_0, \xi} \quad 
    & \frac{1}{2} \| \beta  \|_2^2 +  C \sum_{i=1}^m \xi_i & \\ 
    \textrm{subj. to}  \quad 
    & y_i (x_i^\top \beta + \beta_0) \geq 1 - \xi_i & \forall i = 1, \ldots, m \\
    & \xi_i \geq 0  & \forall i = 1, \ldots, m
\end{aligned}
\end{equation}

Indeed, this minimum is defined iff $\sum \xi_i$ is finite. 


\subsection{Motivation}

In many potential applications of the SVC, there are a large number of training samples (say, millions), 
each of relatively modest dimensionality - i.e. $N$ is much larger than $p$.
While standard solvers for linear support vector classifers perform well in this situation 
\cite{fan2008liblinear}.
the standard \texttt{libSVM} solvers for nonlinear kernels struggle on problems of this 
scale when the data is non-sparse \cite{chang2011libsvm}. \\

The approach presented here uses the alternating descent method of multipliers to derive 
a parallelizable algorithm for fitting support vector classifiers, which will be capable 
of handling this type of large-scale problem.
While only the linear classifier is shown here due to space constraints, 
extending this technique to non-linear kernels is straightforward \cite{forero2010consensus}.


\subsection{Derivation of a distributed algorithm for SVC}

The first goal is to rewrite the SVC problem (\ref{second_form}) in a more convenient form to apply 
the global consensus variable ADMM algorithm. Formulation (\ref{second_form}) is equivalent to \\

\begin{equation*} 
\begin{aligned}
    \argmin_{\beta, \beta_0, \xi} \quad &   \frac{1}{2} \|\beta\|_2^2 + C\sum_{i=1}^m \xi_i\\
    \textrm{subj. to } \quad & \xi_i =  [1 - y_i (\beta^\top x_i + \beta_0)]_+
\end{aligned}
\end{equation*}

where  $[1 - y_i (\beta^\top x_i + \beta_0)]_+ = \max(0, 1 - y_i (\beta^\top x_i + \beta_0))$ . This is then equivalent to

\begin{equation*} 
\begin{aligned}
        \argmin_{\beta, \beta_0, \xi} \quad & \frac{1}{2C} \|\beta\|_2^2 + \sum_{i=1}^m \xi_i\\
        \textrm{subj. to } \quad & \xi_i =  [1 - y_i (\beta^\top x_i + \beta_0)]_+
\end{aligned}
\end{equation*}

Now, minimizing over $\xi$, this problem is equivalent to
\begin{equation} \label{third_formulation}
\begin{aligned}
        \argmin_{\beta, \beta_0} \quad 
        &  \sum_{i=1}^m [1 - y_i (\beta^\top x_i + \beta_0)]_+ \frac{1}{2C} \|\beta\|_2^2 
\end{aligned}
\end{equation}
Indeed, to minimize $\sum \xi_i$ it suffices to take the smallest $\xi_i$ allowed by the constraints,
that is $[1 - y_i (\beta^\top x_i + \beta_0)]_+$. This corresponds to the \emph{penalization method
formulation of the SVC} given in \cite{hastie2009elements}. \\

The problem is now in the desired ``loss + penalty'' form. The term $\sum_{i=1}^n [1 - y_i (\beta^\top x_i + \beta_0)]_+$  is seperable in $x_1, \ldots, x_m$. 
Furthermore, both terms are convex in $\beta$ as $[1 - y_i (\beta^\top x_i + \beta_0)]_+$ is the pointwise maximum of two convex functions. \\

Thus substituting $z = \beta$, 

\begin{equation} 
\begin{aligned}
\argmin_{\beta, \beta_0} \quad 
&  \sum_{i=1}^n [1 - y_i (\beta^\top x_i + \beta_0)]_+ \frac{1}{2C} \|\beta\|_2^2 \\
\textrm{subj. to} \quad & x_i - z_i = 0 \quad \forall i = 1, \ldots, m
\end{aligned}
\end{equation}

This problem can be solved using the global variable consensus ADMM algorithm, 
where $X_i = \begin{bmatrix} x_{i1} & \ldots & x_{im_i} \end{bmatrix}^\top \in \R^{m_i \times p}$ \\

\begin{algorithm}[H]
    \caption{Naive global variable consensus ADMM for SVC}

    \SetAlgoNoLine

    \For{$k=1,2, \ldots$}{
    $ \displaystyle
        (\beta_i)_{k+1}  \leftarrow \argmin_{\beta_i} \left(
            \sum_{j=1}^{m_i} [1 - y_{ij} (\beta_i^\top x_{ij} + \beta_0)]_+ + (\rho/2) \|\beta_i - z_k + (u_i)_k \|_2^2 
        \right) 
    $
    $ \displaystyle
        z_{k+1}  \leftarrow  \argmin_z \left( 
            \frac{1}{2C} \|z\|_2^2 + \frac{N \rho}{2} \|z\|_2^2  - \overline{\beta}_{k+1} - \overline{u}_k
        \right) 
    $
    $ \displaystyle
        (u_i)_{k+1}  \leftarrow (u_i)_k + (\beta_i)_k - z_{k+1}
    $
    }
\end{algorithm}
\vspace{5pt}
This can be simplified further by solving the $z$-update analytically. Rewrite the right-hand 
side as 
\[
    h(z) = \frac{1}{2C} z^\top I z + (z - \overline{\beta}_{k+1} - \overline{u}_k)^\top I (z - \overline{\beta}_{k+1} - \overline{u}_k)
\]
Then, solve 
\begin{align*}
    0 &= \nabla_z h(z)  \\
    0 &= z^\top \left(\frac{1}{c} + N\rho \right) + N\rho \left(- \overline{\beta}_{k+1} - \overline{u}_k \right) \\
    z &= \frac{N \rho}{1/C + N\rho} \left( \overline{\beta}_{k+1} + \overline{u}_k \right)
\end{align*}

This yields the final form of the ADMM algorithm for this problem, \\

\begin{algorithm}[H]
    \caption{Global variable consensus ADMM for SVC}

    \SetAlgoNoLine

    \For{$k=1,2, \ldots$}{
    $ \displaystyle
        (\beta_i)_{k+1}  \leftarrow \argmin_{\beta_i} \left(
            \sum_{j=1}^{m_i} [1 - y_{ij} (\beta_i^\top x_{ij} + \beta_0)]_+ + (\rho/2) \|\beta_i - z_k + (u_i)_k \|_2^2 
        \right) 
    $
    $ \displaystyle
        z_{k+1}  \leftarrow \frac{N\rho}{1/C + N\rho} (\overline{\beta}_{k+1} + \overline{u}_k) 
    $
    $ \displaystyle
        (u_i)_{k+1}  \leftarrow (u_i)_k + (\beta_i)_k - z_{k+1}
    $
    }
\end{algorithm}
\vspace{5pt}
As in the general case, the $u_i$- and $\beta_i$-updates are parallelizable. Notice 
that the minimization problem in the $\beta_i$ subproblems step resembles formulation (\ref{third_formulation}) of the SVC. Indeed, one can treat this this as a modified SVM problem and 
use existing single-process solvers \cite{boydistributed}. \\
This pattern of subproblems having the same form as the original problem appears frequently in 
applications of ADMM. In this sense, ADMM can be seen as a technique to extend nonparallel 
methods to large-scale problems \cite{boydistributed}.

\bibliographystyle{ieeetr}
\bibliography{am229_final_paper}

\end{document}